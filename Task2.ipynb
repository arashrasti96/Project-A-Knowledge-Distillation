{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPT9wrwvpQ4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2c7e87f-fd91-4ceb-e0a6-41927d1927f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.7/dist-packages (0.18.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow_addons) (3.0.9)\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from typing import Union\n",
        "import cv2 as cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import io \n",
        "import pandas as pd\n",
        "import sys\n",
        "tf.enable_v2_behavior()\n",
        "!pip install tensorflow_addons\n",
        "sys.path.append('mhist_dataset/annotations.csv')\n",
        "sys.path.append('mhist_dataset/images.zip')\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 35\n",
        "NUM_CLASSES = 2  # 10 total classes."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "SaJ47SbRct5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile as zp\n",
        "  \n",
        "# specifying the zip file name\n",
        "file_name = \"mhist_dataset/images.zip\"\n",
        "  \n",
        "# opening the zip file in READ mode\n",
        "with zp.ZipFile(file_name, 'r') as z:\n",
        "    # printing all the contents of the zip file\n",
        "    z.printdir()\n",
        "    # extracting all the files\n",
        "    print('Extracting all the files now...')\n",
        "    z.extractall()\n",
        "    print('Done!')"
      ],
      "metadata": {
        "id": "j2PQxdJqqZ4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from os import listdir\n",
        "image_list = []\n",
        "# get the path/directory\n",
        "folder_dir = \"images/\"\n",
        "for images in os.listdir(folder_dir):\n",
        "    # check if the image ends with png\n",
        "    if (images.endswith(\".png\")):\n",
        "        image_list.append(images)"
      ],
      "metadata": {
        "id": "BvlZzS_bul9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_data = np.zeros((len(image_list),224,224,3))\n",
        "for i in range (len(image_list)):\n",
        "  img = cv2.imread('images/'+image_list[i],)\n",
        "  image_data[i,...] = img\n",
        "# cv2_imshow( img)"
      ],
      "metadata": {
        "id": "pSUz6u5bpaRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Normalization\n",
        "for i in range (image_data.shape[0]):\n",
        "  image_data[i] = (image_data[i]-np.min(image_data[i]))/(np.max(image_data[i])-np.min(image_data[i]))  ## normalizing between 0-1"
      ],
      "metadata": {
        "id": "Khp36GF2wSH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('mhist_dataset/annotations.csv')\n",
        "ann_list = df.values.tolist()"
      ],
      "metadata": {
        "id": "a4w-frv_rzrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Train_set = [[],[]]\n",
        "Test_set = [[],[]]\n",
        "cnt_train = 0\n",
        "cnt_test = 0\n",
        "for i in range (len(image_list)):\n",
        "## Collecting Train data samples\n",
        "  if ann_list[i][3] =='train':\n",
        "    idx = image_list.index(ann_list[i][0])\n",
        "    Train_set[0].append(image_data[idx])\n",
        "    if ann_list[i][1] == 'SSA':\n",
        "      Train_set[1].append(tf.constant(([1,0]),dtype=tf.float32))\n",
        "    elif ann_list[i][1] == 'HP':\n",
        "      Train_set[1].append(tf.constant(([0,1]),dtype=tf.float32))\n",
        "  \n",
        "## Collecting Test data samples\n",
        "  elif ann_list[i][3] =='test':\n",
        "    idx = image_list.index(ann_list[i][0])\n",
        "    Test_set[0].append(image_data[idx])\n",
        "    if ann_list[i][1] == 'SSA':\n",
        "      Test_set[1].append(tf.constant(([1,0]),dtype=tf.float32))\n",
        "    elif ann_list[i][1] == 'HP':\n",
        "      Test_set[1].append(tf.constant(([0,1]),dtype=tf.float32))\n"
      ],
      "metadata": {
        "id": "E3Bd_NHGxnSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation"
      ],
      "metadata": {
        "id": "gFeLWd73cpmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_imgs = np.asarray(Train_set[0])\n",
        "train_labels = np.asarray(Train_set[1])\n",
        "test_imgs = np.asarray(Test_set[0])\n",
        "test_labels = np.asarray(Test_set[1])"
      ],
      "metadata": {
        "id": "yhSEVwc8vlZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del Train_set, Test_set, df, image_data, image_list"
      ],
      "metadata": {
        "id": "mL3-aoKxxA2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.RandomRotation(0.2),\n",
        "])"
      ],
      "metadata": {
        "id": "a9KZd7JznKQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 1000\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_imgs,train_labels))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_imgs,test_labels))\n",
        "\n",
        "test_dataset = test_dataset.shuffle(BUFFER_SIZE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
        "## if augmentation is required, uncomment the followings ##################\n",
        "# aug_ds1 = train_dataset.map(\n",
        "#   lambda x, y: (data_augmentation(x, training=True), y))\n",
        "# aug_ds2= train_dataset.map(\n",
        "#   lambda x, y: (data_augmentation(x, training=True), y))\n",
        "# train_dataset = aug_ds2.concatenate(aug_ds1)\n",
        "###########################################################################\n",
        "train_dataset.shuffle(BUFFER_SIZE)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
        "test_size = test_imgs.shape[0]"
      ],
      "metadata": {
        "id": "7VrMH-UVco7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU43ynygJIdi"
      },
      "source": [
        "# Model creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdpnH7LqHd6X"
      },
      "outputs": [],
      "source": [
        "from tensorflow_datasets.core.splits import units\n",
        "from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout,Input\n",
        "\n",
        "resnet50_imagenet_model = ResNet50V2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
        "resnet50_imagenet_model.trainable = False\n",
        "\n",
        "flattened = tf.keras.layers.Flatten()(resnet50_imagenet_model.output)\n",
        "\n",
        "fc1_teacher = tf.keras.layers.Dense(256, activation='relu')(flattened)\n",
        "fc1_teacher = tf.keras.layers.Dense(256, activation='relu')(fc1_teacher)\n",
        "fc1_teacher.trainable = True\n",
        "fc2_teacher = tf.keras.layers.Dense(2, activation='softmax')(fc1_teacher)\n",
        "fc2_teacher.trainable = True\n",
        "teacher_model = tf.keras.models.Model(inputs=resnet50_imagenet_model.input, outputs=fc2_teacher)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## student model\n",
        "mobilenet_imagenet_model = tf.keras.applications.MobileNetV2(include_top = False, input_shape=(224,224,3), weights=\"imagenet\")\n",
        "# mobilenet_imagenet_model.trainable = False\n",
        "fc1_student = flattened = tf.keras.layers.Flatten()(mobilenet_imagenet_model.output)\n",
        "fc1_student = tf.keras.layers.Dense(256, activation='relu')(flattened)\n",
        "fc1_student = tf.keras.layers.Dense(128, activation='relu')(fc1_student)\n",
        "fc2_student = tf.keras.layers.Dense(2, activation='softmax')(fc1_student)\n",
        "student_model = tf.keras.models.Model(inputs=mobilenet_imagenet_model.input, outputs=fc2_student)\n",
        "student_model.save('student_model_untrained.h5')"
      ],
      "metadata": {
        "id": "2oT3_N2qWUj2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e599b441-7d99-4760-e64c-06b49335c0af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teacher loss function"
      ],
      "metadata": {
        "id": "8JWGucyrQGav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @tf.function\n",
        "def compute_teacher_loss(images, labels):\n",
        "  \"\"\"Compute subclass knowledge distillation teacher loss for given images\n",
        "     and labels.\n",
        "\n",
        "  Args:\n",
        "    images: Tensor representing a batch of images.\n",
        "    labels: Tensor representing a batch of labels.\n",
        "\n",
        "  Returns:\n",
        "    Scalar loss Tensor.\n",
        "  \"\"\"\n",
        "  subclass_logits = teacher_model(images, training=True)\n",
        "  # Compute cross-entropy loss for subclasses.\n",
        "  # your code start from here for step 3\n",
        "  cross_entropy_loss_value = tf.reduce_mean(tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(subclass_logits,labels))\n",
        "  return cross_entropy_loss_value"
      ],
      "metadata": {
        "id": "DhzBP6ZLQJ57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Student loss function"
      ],
      "metadata": {
        "id": "JS8xkuH0QbOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters for distillation (need to be tuned).\n",
        "ALPHA =.5  # task balance between cross-entropy and distillation loss\n",
        "DISTILLATION_TEMPERATURE = 4 #temperature hyperparameter\n",
        "\n",
        "def distillation_loss(teacher_logits: tf.Tensor, student_logits: tf.Tensor,\n",
        "                      temperature: Union[float, tf.Tensor]):\n",
        "  \"\"\"Compute distillation loss.\n",
        "\n",
        "  This function computes cross entropy between softened logits and softened\n",
        "  targets. The resulting loss is scaled by the squared temperature so that\n",
        "  the gradient magnitude remains approximately constant as the temperature is\n",
        "  changed. For reference, see Hinton et al., 2014, \"Distilling the knowledge in\n",
        "  a neural network.\"\n",
        "\n",
        "  Args:\n",
        "    teacher_logits: A Tensor of logits provided by the teacher.\n",
        "    student_logits: A Tensor of logits provided by the student, of the same\n",
        "      shape as `teacher_logits`.\n",
        "    temperature: Temperature to use for distillation.\n",
        "\n",
        "  Returns:\n",
        "    A scalar Tensor containing the distillation loss.\n",
        "  \"\"\"\n",
        " # your code start from here for step 3\n",
        "  d = tf.math.exp(teacher_logits/temperature)\n",
        "  n = tf.reduce_sum( tf.math.exp(teacher_logits/temperature))\n",
        "  soft_targets =  d/n\n",
        "\n",
        "  return tf.reduce_mean(\n",
        "      tf.nn.softmax_cross_entropy_with_logits(\n",
        "          soft_targets, student_logits / temperature)) * temperature ** 2\n",
        "\n",
        "def compute_student_loss_with_distil(images, labels):\n",
        "  \"\"\"Compute subclass knowledge distillation student loss for given images\n",
        "     and labels.\n",
        "\n",
        "  Args:\n",
        "    images: Tensor representing a batch of images.\n",
        "    labels: Tensor representing a batch of labels.\n",
        "\n",
        "  Returns:\n",
        "    Scalar loss Tensor.\n",
        "  \"\"\"\n",
        "  student_subclass_logits = student_model(images, training=True)\n",
        "  # Compute subclass distillation loss between student subclass logits and\n",
        "  # softened teacher subclass targets probabilities.\n",
        "  # your code start from here for step 3\n",
        "  teacher_subclass_logits = teacher_model(images, training=False)\n",
        "  distillation_loss_value = distillation_loss(teacher_subclass_logits,student_subclass_logits,DISTILLATION_TEMPERATURE)\n",
        "  # Compute cross-entropy loss with hard targets.\n",
        "  # your code start from here for step 3\n",
        "  cross_entropy_loss_value = tf.reduce_mean(tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(labels,student_subclass_logits))\n",
        "  student_loss = cross_entropy_loss_value + ALPHA*10*distillation_loss_value ## here 10 is the scaling factor. CE is not in between 0-1, therefore, larger scale is required\n",
        "  return student_loss"
      ],
      "metadata": {
        "id": "lDKia4gPQMIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and evaluation"
      ],
      "metadata": {
        "id": "zk1t2XL0ilpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @tf.function\n",
        "def compute_num_correct(model, images, labels):\n",
        "  \"\"\"Compute number of correctly classified images in a batch.\n",
        "\n",
        "  Args:\n",
        "    model: Instance of tf.keras.Model.\n",
        "    images: Tensor representing a batch of images.\n",
        "    labels: Tensor representing a batch of labels.\n",
        "\n",
        "  Returns:\n",
        "    Number of correctly classified images.\n",
        "  \"\"\"\n",
        "  class_logits = model(images, training=False)\n",
        "  return tf.reduce_sum(\n",
        "      tf.cast(tf.math.equal(tf.argmax(class_logits, -1), tf.argmax(labels, -1)),\n",
        "              tf.float32)), tf.argmax(class_logits, -1), tf.argmax(labels, -1)\n",
        "\n",
        "\n",
        "def train_and_evaluate(model, compute_loss_fn):\n",
        "  \"\"\"Perform training and evaluation for a given model.\n",
        "\n",
        "  Args:\n",
        "    model: Instance of tf.keras.Model.\n",
        "    compute_loss_fn: A function that computes the training loss given the\n",
        "      images, and labels.\n",
        "  \"\"\"\n",
        "\n",
        "  # your code start from here for step 4\n",
        "  KD = np.zeros(NUM_EPOCHS)\n",
        "  CE = np.zeros(NUM_EPOCHS)\n",
        "  acc =np.zeros(NUM_EPOCHS)\n",
        "  for epoch in range(1, NUM_EPOCHS + 1):\n",
        "\n",
        "    if epoch <=10:\n",
        "      optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001)\n",
        "    else:\n",
        "      optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001 - 0.0001*0.1)\n",
        "    # Run training.\n",
        "    print('Epoch {}: '.format(epoch), end='')\n",
        "    for images,labels in train_dataset:\n",
        "      \n",
        "      with tf.GradientTape() as tape:\n",
        "         # your code start from here for step 4\n",
        "        loss_value = compute_loss_fn(images,labels)\n",
        "      #grads =\n",
        "      grads = tape.gradient(loss_value, model.trainable_variables) \n",
        "      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    #Run evaluation.\n",
        "    num_correct = 0\n",
        "    num_total = 977\n",
        "    for images, labels in test_dataset:\n",
        "      # your code start from here for step 4\n",
        "      num_correct += compute_num_correct(model,images,labels)[0]\n",
        "    print(\"Class_accuracy: \" + '{:.2f}%'.format(\n",
        "        num_correct / num_total * 100))\n",
        "    acc[epoch-1] = num_correct / num_total * 100\n",
        "    print('Loss',loss_value)\n",
        "    ##########################################################################################\n",
        "    teacher_subclass_logits = teacher_model(images, training=False)\n",
        "    class_logits = model(images, training=False)\n",
        "    CE[epoch-1] = tf.reduce_mean(tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(class_logits,labels))\n",
        "    KD[epoch-1] = distillation_loss(teacher_subclass_logits,class_logits,DISTILLATION_TEMPERATURE)\n",
        "    print(\"KD loss \",KD[epoch-1])\n",
        "    ###########################################################################################\n",
        "  return acc, KD, CE"
      ],
      "metadata": {
        "id": "P2SqhI7agJfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate(teacher_model, compute_teacher_loss)"
      ],
      "metadata": {
        "id": "mEJ864x8Tv6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ALPHA = .5\n",
        "acc_student_distil, KD_student_distil, CE_student_distil = train_and_evaluate(student_model, compute_student_loss_with_distil)"
      ],
      "metadata": {
        "id": "HWYh5uB9A1sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test accuracy vs. tempreture curve"
      ],
      "metadata": {
        "id": "KVzwzpxRPD62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code start from here for step 6\n",
        "Temp_list = [1, 2, 4, 8, 16, 32, 64]\n",
        "ALPHA = .5\n",
        "cnt = 0\n",
        "T = np.zeros(len(Temp_list))\n",
        "for temp in Temp_list:\n",
        "  student_model.load_weights('student_model_untrained.h5')\n",
        "  DISTILLATION_TEMPERATURE = temp \n",
        "  print('############# Training student model with Tempreture = '+ str(temp)+ ' ###########')\n",
        "  train_and_evaluate(student_model,compute_student_loss_with_distil)\n",
        "  #######################\n",
        "  num_correct = 0\n",
        "  num_total = test_size\n",
        "  for images, labels in test_dataset:\n",
        "    # your code start from here for step 4\n",
        "    num_correct += compute_num_correct(student_model,images,labels)[0]\n",
        "  print(\"Class_accuracy: \" + '{:.2f}%'.format(\n",
        "      num_correct / num_total * 100))\n",
        "  T[cnt] = num_correct / num_total * 100\n",
        "  cnt +=1\n",
        "  #######################"
      ],
      "metadata": {
        "id": "hoShTPM6PDla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train student from scratch"
      ],
      "metadata": {
        "id": "iCrIn1nXPp-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mobilenet_imagenet_model = tf.keras.applications.MobileNetV2(include_top = False, input_shape=(224,224,3), weights=\"imagenet\")\n",
        "# mobilenet_imagenet_model.trainable = False\n",
        "fc1_student = flattened = tf.keras.layers.Flatten()(mobilenet_imagenet_model.output)\n",
        "fc1_student = tf.keras.layers.Dense(256, activation='relu')(flattened)\n",
        "fc1_student = tf.keras.layers.Dense(128, activation='relu')(fc1_student)\n",
        "fc2_student = tf.keras.layers.Dense(2, activation='softmax')(fc1_student)\n",
        "student_model_scrach = tf.keras.models.Model(inputs=mobilenet_imagenet_model.input, outputs=fc2_student)\n",
        "\n",
        "def compute_student_loss(images, labels):\n",
        "  \"\"\"Computes the cross entropy loss for the student model without distillation.\n",
        "\n",
        "  Args:\n",
        "    images: Tensor representing a batch of images.\n",
        "    labels: Tensor representing a batch of labels.\n",
        "\n",
        "  Returns:\n",
        "    Scalar loss Tensor.\n",
        "  \"\"\"\n",
        "  student_subclass_logits = student_model(images, training=True)\n",
        "  # Compute cross-entropy loss with hard targets.\n",
        "  cross_entropy_loss_value = tf.reduce_mean(tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(labels,student_subclass_logits))\n",
        "  student_loss = cross_entropy_loss_value \n",
        "  return student_loss\n",
        "  \n",
        "train_and_evaluate(student_model_scrach, compute_student_loss)\n",
        "## in case we want to save the model\n"
      ],
      "metadata": {
        "id": "_fxumhlpAbPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing the teacher and student model (number of of parameters and FLOPs) "
      ],
      "metadata": {
        "id": "fPsMhj04RFtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# your code start from here for step 8\n",
        "!pip install model_profiler\n",
        "from model_profiler import model_profiler\n",
        "# keep order \n",
        "units = ['GPU IDs', 'MFLOPs', 'GB', 'Million', 'MB']\n",
        "\n",
        "Batch_size = 32\n",
        "profile1 = model_profiler(teacher_model, Batch_size,use_units=units,)\n",
        "profile2= model_profiler(student_model, Batch_size,use_units=units,)\n",
        "\n",
        "print(\"The teacher \")\n",
        "print(profile1)\n",
        "print(\"The student \")\n",
        "print(profile2)"
      ],
      "metadata": {
        "id": "7a-BUnK3A9Eu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ba5acd5-3b96-4575-9c07-e794a7e399ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting model_profiler\n",
            "  Downloading model_profiler-1.1.8-py3-none-any.whl (6.4 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from model_profiler) (0.8.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from model_profiler) (1.21.6)\n",
            "Installing collected packages: model-profiler\n",
            "Successfully installed model-profiler-1.1.8\n",
            "| Model Profile                    | Value   | Unit    |\n",
            "|----------------------------------|---------|---------|\n",
            "| Selected GPUs                    | ['0']   | GPU IDs |\n",
            "| No. of FLOPs                     | 69.8616 | MFLOPs  |\n",
            "| GPU Memory Requirement           | 3.7865  | GB      |\n",
            "| Model Parameters                 | 36.4268 | Million |\n",
            "| Memory Required by Model Weights | 138.957 | MB      |\n",
            "| Model Profile                    | Value   | Unit    |\n",
            "|----------------------------------|---------|---------|\n",
            "| Selected GPUs                    | ['0']   | GPU IDs |\n",
            "| No. of FLOPs                     | 5.1966  | MFLOPs  |\n",
            "| GPU Memory Requirement           | 2.6175  | GB      |\n",
            "| Model Parameters                 | 10.303  | Million |\n",
            "| Memory Required by Model Weights | 39.303  | MB      |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/model_profiler/profiler.py:63: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  flops = count_flops(use_units[1], model, Batch_size)\n",
            "/usr/local/lib/python3.7/dist-packages/model_profiler/profiler.py:72: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  np.asarray(values).reshape(-1,1),\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing the state-of-the-art KD algorithm"
      ],
      "metadata": {
        "id": "p_rfKOUhkOiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code start from here for step 5 \n",
        "print('########### Training teacher model with early stop##############')\n",
        "NUM_EPOCHS = 6 ## early stopping the teacher's training\n",
        "teacher_model.load_weights('teacher_model_untrained.h5')\n",
        "acc_teacher_2, _, CE_teacher_2 = train_and_evaluate(teacher_model,compute_teacher_loss)\n",
        "\n",
        "\n",
        "def train_and_evaluate_SOTA_Alg(model, compute_loss_fn):\n",
        "  \"\"\"Perform training and evaluation for a given model.\n",
        "\n",
        "  Args:\n",
        "    model: Instance of tf.keras.Model.\n",
        "    compute_loss_fn: A function that computes the training loss given the\n",
        "      images, and labels.\n",
        "  \"\"\"\n",
        "\n",
        "  # your code start from here for step 4\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "  acc = np.zeros((NUM_EPOCHS))\n",
        "  for epoch in range(1, NUM_EPOCHS + 1):\n",
        "\n",
        "    ## Check if the epoch No. is reached to the half of the total iteration No.\n",
        "    # if epoch >=5:\n",
        "    #   ALPHA = 0 ## Hard-tuning; releasing the student to learn by its own after a while\n",
        "    # Run training.\n",
        "    print('Epoch {}: '.format(epoch), end='')\n",
        "    for images,labels in mnist_train:\n",
        "      \n",
        "      with tf.GradientTape() as tape:\n",
        "         # your code start from here for step 4\n",
        "\n",
        "        loss_value = compute_loss_fn(images,labels)        \n",
        "        #grads =\n",
        "        grads = tape.gradient(loss_value, model.trainable_variables) \n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    #Run evaluation.\n",
        "    num_correct = 0\n",
        "    num_total = builder.info.splits['test'].num_examples\n",
        "    for images, labels in mnist_test:\n",
        "      # your code start from here for step 4\n",
        "      num_correct += compute_num_correct(model,images,labels)[0]\n",
        "    print(\"Class_accuracy: \" + '{:.2f}%'.format(\n",
        "        num_correct / num_total * 100))\n",
        "    acc[epoch-1] = num_correct / num_total * 100\n",
        "  return acc"
      ],
      "metadata": {
        "id": "_qDqJ7bnkPfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "student_model.load_weights('student_model_untrained.h5')\n",
        "train_and_evaluate_SOTA_Alg(student_model,compute_student_loss)"
      ],
      "metadata": {
        "id": "77PhFj75kUTq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
