# Project-A-Knowledge-Distillation
Distilling information from a teacher network to a student network for the purpose of model compression and performance improvement.

The content of this repository is two-fold. In Task 1, we
present the basic concepts of knowledge distillation and test
the algorithm over the MNIST dataset using typical neural network
models as teacher and student models. Then, we employ a SOTA algorithm, named Early-Stopped Knowledge
Distillation to improve the performance of conventional
knowledge distillation. In Task 2, ResNet50V2 and
MobileNetV2 are employed as student and teacher models
for image classification using MHIST dataset.
